{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTS Done\n",
      "Vectorizing Imputs Done\n",
      "Iteration 1, loss = 0.69527793\n",
      "Iteration 2, loss = 0.69337684\n",
      "Iteration 3, loss = 0.69268704\n",
      "Iteration 4, loss = 0.69215315\n",
      "Iteration 5, loss = 0.69134500\n",
      "Iteration 6, loss = 0.69045148\n",
      "Iteration 7, loss = 0.68911474\n",
      "Iteration 8, loss = 0.68774184\n",
      "Iteration 9, loss = 0.68598760\n",
      "Iteration 10, loss = 0.68391805\n",
      "Iteration 11, loss = 0.68149846\n",
      "Iteration 12, loss = 0.67871589\n",
      "Iteration 13, loss = 0.67562964\n",
      "Iteration 14, loss = 0.67224394\n",
      "Iteration 15, loss = 0.66842831\n",
      "Iteration 16, loss = 0.66488453\n",
      "Iteration 17, loss = 0.66127215\n",
      "Iteration 18, loss = 0.65691756\n",
      "Iteration 19, loss = 0.65350339\n",
      "Iteration 20, loss = 0.64938138\n",
      "Iteration 21, loss = 0.64591470\n",
      "Iteration 22, loss = 0.64264942\n",
      "Iteration 23, loss = 0.63866360\n",
      "Iteration 24, loss = 0.63557701\n",
      "Iteration 25, loss = 0.63211613\n",
      "Iteration 26, loss = 0.62897395\n",
      "Iteration 27, loss = 0.62596544\n",
      "Iteration 28, loss = 0.62315281\n",
      "Iteration 29, loss = 0.62001568\n",
      "Iteration 30, loss = 0.61790358\n",
      "Iteration 31, loss = 0.61484585\n",
      "Iteration 32, loss = 0.61243855\n",
      "Iteration 33, loss = 0.60971919\n",
      "Iteration 34, loss = 0.60759043\n",
      "Iteration 35, loss = 0.60524031\n",
      "Iteration 36, loss = 0.60303543\n",
      "Iteration 37, loss = 0.60064851\n",
      "Iteration 38, loss = 0.59885941\n",
      "Iteration 39, loss = 0.59690381\n",
      "Iteration 40, loss = 0.59450275\n",
      "Iteration 41, loss = 0.59256913\n",
      "Iteration 42, loss = 0.59097677\n",
      "Iteration 43, loss = 0.58907963\n",
      "Iteration 44, loss = 0.58802803\n",
      "Iteration 45, loss = 0.58579390\n",
      "Iteration 46, loss = 0.58372617\n",
      "Iteration 47, loss = 0.58228507\n",
      "Iteration 48, loss = 0.58073621\n",
      "Iteration 49, loss = 0.57885454\n",
      "Iteration 50, loss = 0.57764319\n",
      "Iteration 51, loss = 0.57632899\n",
      "Iteration 52, loss = 0.57476353\n",
      "Iteration 53, loss = 0.57289048\n",
      "Iteration 54, loss = 0.57181938\n",
      "Iteration 55, loss = 0.57088528\n",
      "Iteration 56, loss = 0.56952102\n",
      "Iteration 57, loss = 0.56788132\n",
      "Iteration 58, loss = 0.56635326\n",
      "Iteration 59, loss = 0.56532259\n",
      "Iteration 60, loss = 0.56461637\n",
      "Iteration 61, loss = 0.56354583\n",
      "Iteration 62, loss = 0.56185969\n",
      "Iteration 63, loss = 0.56076457\n",
      "Iteration 64, loss = 0.56016466\n",
      "Iteration 65, loss = 0.55891847\n",
      "Iteration 66, loss = 0.55793578\n",
      "Iteration 67, loss = 0.55725427\n",
      "Iteration 68, loss = 0.55639305\n",
      "Iteration 69, loss = 0.55520795\n",
      "Iteration 70, loss = 0.55362406\n",
      "Iteration 71, loss = 0.55344094\n",
      "Iteration 72, loss = 0.55178090\n",
      "Iteration 73, loss = 0.55091501\n",
      "Iteration 74, loss = 0.54999849\n",
      "Iteration 75, loss = 0.54938958\n",
      "Iteration 76, loss = 0.54830236\n",
      "Iteration 77, loss = 0.54714643\n",
      "Iteration 78, loss = 0.54688346\n",
      "Iteration 79, loss = 0.54575951\n",
      "Iteration 80, loss = 0.54437052\n",
      "Iteration 81, loss = 0.54414236\n",
      "Iteration 82, loss = 0.54305568\n",
      "Iteration 83, loss = 0.54219889\n",
      "Iteration 84, loss = 0.54170112\n",
      "Iteration 85, loss = 0.54073762\n",
      "Iteration 86, loss = 0.53970721\n",
      "Iteration 87, loss = 0.53883525\n",
      "Iteration 88, loss = 0.53808986\n",
      "Iteration 89, loss = 0.53818073\n",
      "Iteration 90, loss = 0.53691536\n",
      "Iteration 91, loss = 0.53615785\n",
      "Iteration 92, loss = 0.53586122\n",
      "Iteration 93, loss = 0.53476677\n",
      "Iteration 94, loss = 0.53434265\n",
      "Iteration 95, loss = 0.53310786\n",
      "Iteration 96, loss = 0.53311135\n",
      "Iteration 97, loss = 0.53182172\n",
      "Iteration 98, loss = 0.53112222\n",
      "Iteration 99, loss = 0.53066193\n",
      "Iteration 100, loss = 0.53047660\n",
      "Iteration 101, loss = 0.52941141\n",
      "Iteration 102, loss = 0.53016902\n",
      "Iteration 103, loss = 0.52817934\n",
      "Iteration 104, loss = 0.52767246\n",
      "Iteration 105, loss = 0.52650858\n",
      "Iteration 106, loss = 0.52614407\n",
      "Iteration 107, loss = 0.52597440\n",
      "Iteration 108, loss = 0.52500795\n",
      "Iteration 109, loss = 0.52495899\n",
      "Iteration 110, loss = 0.52427414\n",
      "Iteration 111, loss = 0.52327519\n",
      "Iteration 112, loss = 0.52278452\n",
      "Iteration 113, loss = 0.52343680\n",
      "Iteration 114, loss = 0.52301477\n",
      "Iteration 115, loss = 0.52140394\n",
      "Iteration 116, loss = 0.52078765\n",
      "Iteration 117, loss = 0.52072535\n",
      "Iteration 118, loss = 0.51977706\n",
      "Iteration 119, loss = 0.51930275\n",
      "Iteration 120, loss = 0.51876831\n",
      "Iteration 121, loss = 0.51795884\n",
      "Iteration 122, loss = 0.51731388\n",
      "Iteration 123, loss = 0.51760704\n",
      "Iteration 124, loss = 0.51600133\n",
      "Iteration 125, loss = 0.51649717\n",
      "Iteration 126, loss = 0.51605097\n",
      "Iteration 127, loss = 0.51487652\n",
      "Iteration 128, loss = 0.51460639\n",
      "Iteration 129, loss = 0.51408720\n",
      "Iteration 130, loss = 0.51295789\n",
      "Iteration 131, loss = 0.51361569\n",
      "Iteration 132, loss = 0.51221655\n",
      "Iteration 133, loss = 0.51242731\n",
      "Iteration 134, loss = 0.51206070\n",
      "Iteration 135, loss = 0.51119425\n",
      "Iteration 136, loss = 0.51033227\n",
      "Iteration 137, loss = 0.51053188\n",
      "Iteration 138, loss = 0.51015300\n",
      "Iteration 139, loss = 0.50930817\n",
      "Iteration 140, loss = 0.50928863\n",
      "Iteration 141, loss = 0.50904810\n",
      "Iteration 142, loss = 0.50832017\n",
      "Iteration 143, loss = 0.50702990\n",
      "Iteration 144, loss = 0.50814423\n",
      "Iteration 145, loss = 0.50636813\n",
      "Iteration 146, loss = 0.50610380\n",
      "Iteration 147, loss = 0.50655275\n",
      "Iteration 148, loss = 0.50548334\n",
      "Iteration 149, loss = 0.50501246\n",
      "Iteration 150, loss = 0.50623917\n",
      "Iteration 151, loss = 0.50397214\n",
      "Iteration 152, loss = 0.50343278\n",
      "Iteration 153, loss = 0.50275151\n",
      "Iteration 154, loss = 0.50304724\n",
      "Iteration 155, loss = 0.50337486\n",
      "Iteration 156, loss = 0.50287777\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import*\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Hyper Parameters\n",
    "train_set_pct = 0.9\n",
    "\n",
    "def tts(csvReader):\n",
    "    X = []\n",
    "    Y = []\n",
    "    IDs = []\n",
    "    \n",
    "    for row in reader:\n",
    "        X.append(row['text'])\n",
    "        Y.append(row['label'])\n",
    "        IDs.append(row['docID'])\n",
    "    \n",
    "    l = len(X)\n",
    "    n = int(train_set_pct*l)\n",
    "        \n",
    "    tr_x = X[0:n]\n",
    "    tr_y = Y[0:n]\n",
    "    tr_id = IDs[0:n]\n",
    "    te_x = X[n:]\n",
    "    te_y = Y[n:]\n",
    "    te_id = IDs[n:]\n",
    "        \n",
    "    tr_y = [int(n) for n in tr_y]    \n",
    "    te_y = [int(n) for n in te_y]    \n",
    "    \n",
    "    return tr_x, tr_y, tr_id, te_x, te_y, te_id\n",
    "    \n",
    "        \n",
    "#Train Test Split Those Boyos--------------------\n",
    "with open('shuffled_clean_data_tokenized.csv', 'r', encoding = 'utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    tr_x, tr_y, tr_id, te_x, te_y, te_id = tts(reader)\n",
    "print(\"TTS Done\")\n",
    "\n",
    "#Turns those documents in X into vectors--------------\n",
    "d2v = gensim.models.Doc2Vec.load(\"doc2vec600.model\")\n",
    "tr_x = [d2v.docvecs[i] for i in range(len(tr_x))]\n",
    "te_x = [d2v.docvecs[i] for i in range(len(te_x))]\n",
    "print(\"Vectorizing Imputs Done\")\n",
    "\n",
    "\n",
    "X = tr_x \n",
    "y = tr_y\n",
    "clf = MLPClassifier(alpha=1e-7, hidden_layer_sizes=(700, 3), verbose = True, shuffle=False, batch_size = 1000)\n",
    "clf.fit(X, y)\n",
    "print(\"Training Done\")\n",
    "\n",
    "X = te_x\n",
    "y = te_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp,fp,tn,fn: 1077611089100939688\n",
      "0.49284244225931856\n",
      "0.5265832681782643\n",
      "0.5091544803798815\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('NN_Classifier_700By3.Model', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "with open('testing_set.txt', 'wb') as f:\n",
    "    pickle.dump(X,f)\n",
    "with open('testing_labels.txt', 'wb') as f:\n",
    "    pickle.dump(y,f)\n",
    "with open('testing_Ids.txt', 'wb') as f:\n",
    "    pickle.dump(te_id,f)\n",
    "p = clf.predict(X)\n",
    "\n",
    "t_p = 0\n",
    "t_n = 0\n",
    "f_p = 0\n",
    "f_n = 0\n",
    "\n",
    "\n",
    "for n in range(len(y)):\n",
    "    if p[n] == y[n]:\n",
    "        if p[n] == 1:\n",
    "            t_p += 1;\n",
    "        else:\n",
    "            t_n += 1\n",
    "    \n",
    "    else:\n",
    "        if p[n] == 0:\n",
    "            f_n += 1;\n",
    "        else:\n",
    "            f_p += 1;\n",
    "            \n",
    "if t_p+f_p == 0:\n",
    "    precision = 0\n",
    "else:\n",
    "    precision = t_p / (t_p + f_p);\n",
    "recall = t_p / (t_p + f_n);\n",
    "if recall == 0:\n",
    "    f_score = 0\n",
    "else:\n",
    "    f_score = (2 * precision * recall) / (precision + recall);\n",
    "print(\"tp,fp,tn,fn: \" + str(t_p) + str(f_p) + str(t_n) + str(f_n))\n",
    "print(precision);\n",
    "print(recall);\n",
    "print(f_score);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
